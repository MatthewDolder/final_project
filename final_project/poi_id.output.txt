Run Decision Tree Classifer to choose best features: 
{'bonus': 11, 'deferred_income': 2, 'to_messages': 1, 'total_stock_value': 1, 'other': 3, 'exercised_stock_options': 4, 'total_payments': 1, 'restricted_stock': 1, 'expenses': 3, 'shared_receipt_with_poi': 1, 'long_term_incentive': 1, 'ratio_to': 1} 


Try the three best features with GaussianNB()
GNB: {'accuracy': 0.806060606060606, 'precision': 0.358974358974359, 'recall': 0.11965811965811966, 'f1': 0.1794871794871795, 'scores': {'tp': 14, 'tn': 518, 'fp': 25, 'fn': 103}} 


Try Decision Tree Classifer with 3 best features
DTC: {'accuracy': 0.7727272727272727, 'precision': 0.36363636363636365, 'recall': 0.37606837606837606, 'f1': 0.3697478991596639, 'scores': {'tp': 44, 'tn': 466, 'fp': 77, 'fn': 73}} 


Try SVC(kernel='rbf',C=1000,gamma='auto') with 3 best features
SVC_rbf - auto: {'accuracy': 0.8227272727272728, 'precision': 0, 'recall': 0, 'f1': 0, 'scores': {'tp': 0, 'tn': 543, 'fp': 0, 'fn': 117}} 


Try SVC(kernel='rbf',C=1000,gamma='scale') with 3 best features
SVC_rbf - scale: {'accuracy': 0.7803030303030303, 'precision': 0.325, 'recall': 0.2222222222222222, 'f1': 0.2639593908629442, 'scores': {'tp': 26, 'tn': 489, 'fp': 54, 'fn': 91}} 


Try SVC(kernel='linear') with 3 best features
Note: this ran for a long time without finishing.  My guess is because the features are not scaled and they are     failing to converge.  I commented this out. 


Try LinearSVC() with 3 best features
Note: this ran quickly, but threw converge errors.  Commenting this out. 


Test GaussianNB() with outliers removed.
GNB: {'accuracy': 0.843939393939394, 'precision': 0.33962264150943394, 'recall': 0.20930232558139536, 'f1': 0.2589928057553957, 'scores': {'tp': 18, 'tn': 539, 'fp': 35, 'fn': 68}} 


Test DTC with outliers removed.
DTC: {'accuracy': 0.8106060606060606, 'precision': 0.32432432432432434, 'recall': 0.4186046511627907, 'f1': 0.3654822335025381, 'scores': {'tp': 36, 'tn': 499, 'fp': 75, 'fn': 50}} 


Test custom features along with bonus with GNB and DTC
GNB - ratio - small: {'accuracy': 0.825, 'precision': 0.38596491228070173, 'recall': 0.2391304347826087, 'f1': 0.29530201342281875, 'scores': {'tp': 22, 'tn': 473, 'fp': 35, 'fn': 70}}
DTC - ratio - small: {'accuracy': 0.785, 'precision': 0.29213483146067415, 'recall': 0.2826086956521739, 'f1': 0.287292817679558, 'scores': {'tp': 26, 'tn': 445, 'fp': 63, 'fn': 66}} 


Test custom features along with bonus with GNB and DTC - outlier removed
GNB - ratio - small: {'accuracy': 0.845, 'precision': 0.4318181818181818, 'recall': 0.21839080459770116, 'f1': 0.29007633587786263, 'scores': {'tp': 19, 'tn': 488, 'fp': 25, 'fn': 68}}
DTC - ratio - small: {'accuracy': 0.76, 'precision': 0.2336448598130841, 'recall': 0.28735632183908044, 'f1': 0.25773195876288657, 'scores': {'tp': 25, 'tn': 431, 'fp': 82, 'fn': 62}} 


Test LinearSVC() with scaled features.
LinearSVC: {'accuracy': 0.8212121212121212, 'precision': 0.4444444444444444, 'recall': 0.03418803418803419, 'f1': 0.0634920634920635, 'scores': {'tp': 4, 'tn': 538, 'fp': 5, 'fn': 113}} 


Use GridSearchCV to find best parameter for GaussianNB:
{'var_smoothing': 0.1} 


Test GNB with var_smoothing=0.1:
GNB - var_smoothing=1: {'accuracy': 0.8121212121212121, 'precision': 0.3870967741935484, 'recall': 0.10256410256410256, 'f1': 0.16216216216216214, 'scores': {'tp': 12, 'tn': 524, 'fp': 19, 'fn': 105}} 


Use GridSearchCV to find best parameter for DTC:
{'min_samples_split': 2, 'splitter': 'random'} 


Test DTC with min_samples_split=5
DTC: {'accuracy': 0.7803030303030303, 'precision': 0.3541666666666667, 'recall': 0.2905982905982906, 'f1': 0.31924882629107987, 'scores': {'tp': 34, 'tn': 481, 'fp': 62, 'fn': 83}} 


Use GridSearchCV to find best parameter for SVC:
{'gamma': 1}
Test SVC with gamma=1
SVC_rbf - gamma 1: {'accuracy': 0.8227272727272728, 'precision': 0, 'recall': 0, 'f1': 0, 'scores': {'tp': 0, 'tn': 543, 'fp': 0, 'fn': 117}} 






Best Score.  DTC() with 'bonus','expenses','other'
DTC: {'accuracy': 0.7848484848484848, 'precision': 0.3893805309734513, 'recall': 0.37606837606837606, 'f1': 0.38260869565217387, 'scores': {'tp': 44, 'tn': 474, 'fp': 69, 'fn': 73}} 



Metrics:
dataset length  145
true labels: 18
false labels: 92
features used: 3
*bonus* empty:  0.4413793103448276
*expenses* empty:  0.35172413793103446
*other* empty:  0.36551724137931035


DecisionTreeClassifier()
	Accuracy: 0.76309	Precision: 0.34036	Recall: 0.32300	F1: 0.33145	F2: 0.32633
	Total predictions: 11000	True positives:  646	False positives: 1252	False negatives: 1354	True negatives: 7748

Python version   3.8.9 (tags/v3.8.9:a743f81, Apr  6 2021, 14:02:34) [MSC v.1928 64 bit (AMD64)]
sklearn version   0.24.2